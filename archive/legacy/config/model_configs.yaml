# Model-specific configurations
microsoft/phi-2:
  temperature: 1.0  # Use default temperature since we're not sampling
  do_sample: false  # Disable sampling for deterministic outputs
  return_token_type_ids: false

nilq/mistral-1L-tiny:
  return_token_type_ids: false

EleutherAI/pythia-70m:
  return_token_type_ids: false
  temperature: 1.0  # Use default temperature
  do_sample: false  # Disable sampling to avoid probability issues
  num_beams: 1      # Use greedy decoding

TinyLlama/TinyLlama-1.1B-Chat-v1.0:
  return_token_type_ids: false
  temperature: 0.7  # Slightly lower temperature for more focused outputs
  do_sample: true   # Enable sampling for more natural responses

# GGUF Model Configurations
Qwen3-0.6B-Q4_K_M.gguf:
  temperature: 0.7
  n_ctx: 2048
  n_threads: 4
  n_gpu_layers: 0
  n_batch: 512
  repeat_penalty: 1.1
  top_k: 40
  top_p: 0.95
  stop: ["</s>", "}"]  # Stop at end of sentence or JSON
  f16_kv: true  # Use float16 for key/value cache
  embedding: false  # Don't use embedding mode
  vocab_only: false  # Load full model
  use_mmap: true  # Use memory mapping
  use_mlock: false  # Don't lock memory

Qwen3-1.7B-Q4_K_M.gguf:
  temperature: 0.7
  n_ctx: 2048
  n_threads: 4
  n_gpu_layers: 0
  n_batch: 512
  repeat_penalty: 1.1
  top_k: 40
  top_p: 0.95
  stop: ["</s>", "}"]  # Stop at end of sentence or JSON
  f16_kv: true  # Use float16 for key/value cache
  embedding: false  # Don't use embedding mode
  vocab_only: false  # Load full model
  use_mmap: true  # Use memory mapping
  use_mlock: false  # Don't lock memory

# Add more model configurations below 