2025-04-04 12:53:04,948 - voice-service - INFO - Using device: cuda
2025-04-04 12:53:04,967 - voice-service - INFO - GPU: Orin
2025-04-04 12:53:04,967 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 12:53:04,967 - voice-service - INFO - Using models directory: /models
2025-04-04 12:53:04,967 - voice-service - INFO - Using config directory: /app/config
2025-04-04 12:53:04,990 - voice-service - INFO - Starting server...
2025-04-04 13:17:34,179 - voice-service - INFO - Using device: cuda
2025-04-04 13:17:34,199 - voice-service - INFO - GPU: Orin
2025-04-04 13:17:34,199 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 13:17:34,199 - voice-service - INFO - Using models directory: /models
2025-04-04 13:17:34,200 - voice-service - INFO - Using config directory: /app/config
2025-04-04 13:17:34,224 - voice-service - INFO - Starting server...
2025-04-04 13:24:53,312 - voice-service - INFO - Using device: cuda
2025-04-04 13:24:53,332 - voice-service - INFO - GPU: Orin
2025-04-04 13:24:53,332 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 13:24:53,333 - voice-service - INFO - Using models directory: /models
2025-04-04 13:24:53,333 - voice-service - INFO - Using config directory: /app/config
2025-04-04 13:24:53,355 - voice-service - INFO - Starting server...
2025-04-04 13:32:56,511 - voice-service - INFO - Using device: cuda
2025-04-04 13:32:56,532 - voice-service - INFO - GPU: Orin
2025-04-04 13:32:56,533 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 13:32:56,533 - voice-service - INFO - Using models directory: /models
2025-04-04 13:32:56,533 - voice-service - INFO - Using config directory: /app/config
2025-04-04 13:32:56,556 - voice-service - INFO - Starting server...
2025-04-04 13:40:34,765 - voice-service - INFO - Using device: cuda
2025-04-04 13:40:34,784 - voice-service - INFO - GPU: Orin
2025-04-04 13:40:34,784 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 13:40:34,784 - voice-service - INFO - Using models directory: /models
2025-04-04 13:40:34,784 - voice-service - INFO - Using config directory: /app/config
2025-04-04 13:40:34,807 - voice-service - INFO - Starting server...
2025-04-04 13:41:28,691 - voice-service - INFO - Requested model: tinyllama, using: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 13:41:28,692 - voice-service - INFO - Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 13:41:28,692 - voice-service - INFO - Step 1: Loading tokenizer for TinyLlama/TinyLlama-1.1B-Chat-v1.0...
2025-04-04 13:41:28,880 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 13:41:28,880 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 13:41:28,880 - voice-service - INFO - Tokenizer doesn't have vocabulary attribute, adding it...
2025-04-04 13:41:28,901 - voice-service - INFO - Added vocabulary from get_vocab with 32000 items
2025-04-04 13:41:28,902 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '</s>' and tokenizer attributes fixed.
2025-04-04 13:41:28,902 - voice-service - INFO - Step 3: Loading model weights for TinyLlama/TinyLlama-1.1B-Chat-v1.0...
2025-04-04 13:41:29,014 - voice-service - INFO - Available memory before loading model: 3.95 GB
2025-04-04 13:41:35,962 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-04 13:41:40,802 - voice-service - INFO - Model moved to device: cuda
2025-04-04 13:41:40,803 - voice-service - INFO - Available memory after loading model: 1.68 GB
2025-04-04 13:41:40,803 - voice-service - INFO - Memory used by model: 2.27 GB
2025-04-04 13:41:40,803 - voice-service - INFO - Step 3 COMPLETE: Model weights loaded.
2025-04-04 13:41:40,803 - voice-service - INFO - Successfully loaded model: TinyLlama/TinyLlama-1.1B-Chat-v1.0 (Type: llama)
2025-04-04 13:45:17,606 - voice-service - INFO - Received request: {'prompt': 'turn on the lights in the bedroom', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 13:45:17,606 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 13:45:17,606 - voice-service - INFO - Unloading current model TinyLlama/TinyLlama-1.1B-Chat-v1.0 to load distilgpt2
2025-04-04 13:45:17,607 - voice-service - INFO - Unloading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 13:45:18,204 - voice-service - INFO - Successfully unloaded model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 13:45:18,206 - voice-service - INFO - Loading model: distilgpt2
2025-04-04 13:45:18,206 - voice-service - INFO - Step 1: Loading tokenizer for distilgpt2...
2025-04-04 13:45:18,496 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 13:45:18,497 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 13:45:18,497 - voice-service - INFO - Tokenizer doesn't have vocabulary attribute, adding it...
2025-04-04 13:45:18,530 - voice-service - INFO - Added vocabulary from get_vocab with 50257 items
2025-04-04 13:45:18,530 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '<|endoftext|>' and tokenizer attributes fixed.
2025-04-04 13:45:18,530 - voice-service - INFO - Step 3: Loading model weights for distilgpt2...
2025-04-04 13:45:18,531 - voice-service - INFO - Available memory before loading model: 3.01 GB
2025-04-04 13:45:18,605 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-04 13:45:19,056 - voice-service - INFO - Model moved to device: cuda
2025-04-04 13:45:19,057 - voice-service - INFO - Available memory after loading model: 2.90 GB
2025-04-04 13:45:19,057 - voice-service - INFO - Memory used by model: 0.11 GB
2025-04-04 13:45:19,057 - voice-service - INFO - Step 3 COMPLETE: Model weights loaded.
2025-04-04 13:45:19,057 - voice-service - INFO - Successfully loaded model: distilgpt2 (Type: gpt2)
2025-04-04 13:45:19,057 - voice-service - INFO - Created prompt: Convert this command to JSON: "turn on the lights in the bedroom"

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 13:45:19,058 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 13:45:22,985 - voice-service - INFO - Raw generation complete: {


"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 3:
Comma...
2025-04-04 13:45:22,985 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 13:45:22,986 - outlines-service - INFO - Successfully extracted JSON: {'device': 'lights', 'location': 'kitchen', 'action': 'turn_on', 'value': None}
2025-04-04 13:45:23,002 - voice-service - INFO - Returning response with command: {"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}
2025-04-04 13:47:35,464 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 13:47:35,465 - voice-service - INFO - Loading model: distilgpt2
2025-04-04 13:47:35,465 - voice-service - INFO - Step 1: Loading tokenizer for distilgpt2...
2025-04-04 13:47:35,726 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 13:47:35,726 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 13:47:35,726 - voice-service - INFO - Tokenizer doesn't have vocabulary attribute, adding it...
2025-04-04 13:47:35,758 - voice-service - INFO - Added vocabulary from get_vocab with 50257 items
2025-04-04 13:47:35,758 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '<|endoftext|>' and tokenizer attributes fixed.
2025-04-04 13:47:35,759 - voice-service - INFO - Step 3: Loading model weights for distilgpt2...
2025-04-04 13:47:35,759 - voice-service - INFO - Available memory before loading model: 2.49 GB
2025-04-04 13:47:35,795 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-04 13:47:36,136 - voice-service - INFO - Model moved to device: cuda
2025-04-04 13:47:36,137 - voice-service - INFO - Available memory after loading model: 2.26 GB
2025-04-04 13:47:36,137 - voice-service - INFO - Memory used by model: 0.23 GB
2025-04-04 13:47:36,137 - voice-service - INFO - Step 3 COMPLETE: Model weights loaded.
2025-04-04 13:47:36,157 - voice-service - INFO - Successfully loaded model: distilgpt2 (Type: gpt2)
2025-04-04 13:47:52,796 - voice-service - INFO - Received request: {'prompt': 'turn on the lights in the bedroom', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 13:47:52,797 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 13:47:52,797 - voice-service - INFO - Using cached model: distilgpt2
2025-04-04 13:47:52,798 - voice-service - INFO - Created prompt: Convert this command to JSON: "turn on the lights in the bedroom"

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 13:47:52,798 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 13:47:55,259 - voice-service - INFO - Raw generation complete: {
2025-04-04 13:47:55,260 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 13:47:55,261 - outlines-service - WARNING - Failed to extract valid JSON or fields
2025-04-04 13:47:55,261 - voice-service - WARNING - Regex parsing failed to extract valid JSON
2025-04-04 13:47:55,261 - voice-service - INFO - Returning response with command: None
2025-04-04 13:48:05,761 - voice-service - INFO - Received request: {'prompt': 'turn on the bedroom lights', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 13:48:05,761 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 13:48:05,761 - voice-service - INFO - Using cached model: distilgpt2
2025-04-04 13:48:05,762 - voice-service - INFO - Created prompt: Convert this command to JSON: "turn on the bedroom lights"

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 13:48:05,762 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 13:48:08,228 - voice-service - INFO - Raw generation complete: {
2025-04-04 13:48:08,228 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 13:48:08,228 - outlines-service - WARNING - Failed to extract valid JSON or fields
2025-04-04 13:48:08,228 - voice-service - WARNING - Regex parsing failed to extract valid JSON
2025-04-04 13:48:08,228 - voice-service - INFO - Returning response with command: None
2025-04-04 13:53:27,547 - voice-service - INFO - Using device: cuda
2025-04-04 13:53:27,566 - voice-service - INFO - GPU: Orin
2025-04-04 13:53:27,566 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 13:53:27,567 - voice-service - INFO - Using models directory: /models
2025-04-04 13:53:27,567 - voice-service - INFO - Using config directory: /app/config
2025-04-04 13:53:27,593 - voice-service - INFO - Starting server...
2025-04-04 13:55:00,176 - voice-service - INFO - Requested model: gpt2, using: gpt2
2025-04-04 13:55:00,177 - voice-service - INFO - Loading model: gpt2
2025-04-04 13:55:00,177 - voice-service - INFO - Step 1: Loading tokenizer for gpt2...
2025-04-04 13:55:00,482 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 13:55:00,483 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 13:55:00,483 - voice-service - INFO - Tokenizer doesn't have vocabulary attribute, adding it...
2025-04-04 13:55:00,514 - voice-service - INFO - Added vocabulary from get_vocab with 50257 items
2025-04-04 13:55:00,515 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '<|endoftext|>' and tokenizer attributes fixed.
2025-04-04 13:55:00,515 - voice-service - INFO - Step 3: Loading model weights for gpt2...
2025-04-04 13:55:00,601 - voice-service - INFO - Available memory before loading model: 3.33 GB
2025-04-04 14:10:06,764 - voice-service - INFO - Using device: cuda
2025-04-04 14:10:06,786 - voice-service - INFO - GPU: Orin
2025-04-04 14:10:06,786 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 14:10:06,786 - voice-service - INFO - Using models directory: /models
2025-04-04 14:10:06,786 - voice-service - INFO - Using config directory: /app/config
2025-04-04 14:10:06,810 - voice-service - INFO - Starting server...
2025-04-04 14:10:09,918 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 14:10:09,918 - voice-service - INFO - Loading model: distilgpt2
2025-04-04 14:10:09,918 - voice-service - INFO - Step 1: Loading tokenizer for distilgpt2...
2025-04-04 14:10:10,213 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 14:10:10,213 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 14:10:10,213 - voice-service - INFO - Tokenizer doesn't have vocabulary attribute, adding it...
2025-04-04 14:10:10,245 - voice-service - INFO - Added vocabulary from get_vocab with 50257 items
2025-04-04 14:10:10,245 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '<|endoftext|>' and tokenizer attributes fixed.
2025-04-04 14:10:10,245 - voice-service - INFO - Step 3: Loading model weights for distilgpt2...
2025-04-04 14:10:10,321 - voice-service - INFO - Available memory before loading model: 3.76 GB
2025-04-04 14:10:17,175 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-04 14:10:17,533 - voice-service - INFO - Model moved to device: cuda
2025-04-04 14:10:17,534 - voice-service - INFO - Available memory after loading model: 3.46 GB
2025-04-04 14:10:17,534 - voice-service - INFO - Memory used by model: 0.29 GB
2025-04-04 14:10:17,534 - voice-service - INFO - Step 3 COMPLETE: Model weights loaded.
2025-04-04 14:10:17,534 - voice-service - INFO - Successfully loaded model: distilgpt2 (Type: gpt2)
2025-04-04 14:10:35,480 - voice-service - INFO - Received request: {'prompt': 'turn on the kitchen lights', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 14:10:35,480 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 14:10:35,480 - voice-service - INFO - Using cached model: distilgpt2
2025-04-04 14:10:35,481 - voice-service - INFO - Created prompt: Convert this command to JSON: "turn on the kitchen lights"

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 14:10:35,481 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 14:10:39,371 - voice-service - INFO - Raw generation complete: {
2025-04-04 14:10:39,372 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 14:10:39,373 - outlines-service - WARNING - Failed to extract valid JSON or fields
2025-04-04 14:10:39,373 - voice-service - WARNING - Regex parsing failed to extract valid JSON
2025-04-04 14:10:39,373 - voice-service - INFO - Returning response with command: None
2025-04-04 14:13:40,501 - voice-service - INFO - Using device: cuda
2025-04-04 14:13:40,520 - voice-service - INFO - GPU: Orin
2025-04-04 14:13:40,520 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 14:13:40,520 - voice-service - INFO - Using models directory: /models
2025-04-04 14:13:40,520 - voice-service - INFO - Using config directory: /app/config
2025-04-04 14:13:40,543 - voice-service - INFO - Starting server...
2025-04-04 14:14:05,063 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 14:14:05,063 - voice-service - INFO - Loading model: distilgpt2
2025-04-04 14:14:05,064 - voice-service - INFO - Step 1: Loading tokenizer for distilgpt2...
2025-04-04 14:14:05,361 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 14:14:05,362 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 14:14:05,362 - voice-service - INFO - Tokenizer doesn't have vocabulary attribute, adding it...
2025-04-04 14:14:05,393 - voice-service - INFO - Added vocabulary from get_vocab with 50257 items
2025-04-04 14:14:05,394 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '<|endoftext|>' and tokenizer attributes fixed.
2025-04-04 14:14:05,394 - voice-service - INFO - Step 3: Loading model weights for distilgpt2...
2025-04-04 14:14:05,469 - voice-service - INFO - Available memory before loading model: 3.60 GB
2025-04-04 14:14:11,417 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-04 14:14:11,729 - voice-service - INFO - Model moved to device: cuda
2025-04-04 14:14:11,730 - voice-service - INFO - Available memory after loading model: 3.40 GB
2025-04-04 14:14:11,730 - voice-service - INFO - Memory used by model: 0.20 GB
2025-04-04 14:14:11,730 - voice-service - INFO - Step 3 COMPLETE: Model weights loaded.
2025-04-04 14:14:11,730 - voice-service - INFO - Successfully loaded model: distilgpt2 (Type: gpt2)
2025-04-04 14:14:24,653 - voice-service - INFO - Received request: {'prompt': 'turn on the kitchen lights', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 14:14:24,654 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 14:14:24,654 - voice-service - INFO - Using cached model: distilgpt2
2025-04-04 14:14:24,655 - voice-service - INFO - Created prompt: Convert this command to JSON: "turn on the kitchen lights"

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 14:14:24,655 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 14:14:28,555 - voice-service - INFO - Raw generation complete: {
2025-04-04 14:14:28,556 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 14:14:28,557 - outlines-service - WARNING - Failed to extract valid JSON or fields
2025-04-04 14:14:28,557 - voice-service - WARNING - Regex parsing failed to extract valid JSON
2025-04-04 14:14:28,557 - voice-service - INFO - Returning response with command: None
2025-04-04 15:51:21,785 - voice-service - INFO - Received request: {'prompt': 'turn on the kitchen lights --max_tokens 200', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 15:51:21,786 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 15:51:21,786 - voice-service - INFO - Using cached model: distilgpt2
2025-04-04 15:51:21,786 - voice-service - INFO - Created prompt: Convert this command to JSON: "turn on the kitchen lights --max_tokens 200"

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 15:51:21,787 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 15:51:24,178 - voice-service - INFO - Raw generation complete: {
2025-04-04 15:51:24,178 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 15:51:24,178 - outlines-service - WARNING - Failed to extract valid JSON or fields
2025-04-04 15:51:24,179 - voice-service - WARNING - Regex parsing failed to extract valid JSON
2025-04-04 15:51:24,179 - voice-service - INFO - Returning response with command: None
2025-04-04 15:51:24,307 - voice-service - INFO - Received request: {'prompt': '', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 15:51:24,308 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 15:51:24,308 - voice-service - INFO - Using cached model: distilgpt2
2025-04-04 15:51:24,308 - voice-service - INFO - Created prompt: Convert this command to JSON: ""

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 15:51:24,309 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 15:51:26,634 - voice-service - INFO - Raw generation complete: {


}
}

}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
2025-04-04 15:51:26,634 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 15:51:26,642 - outlines-service - WARNING - Failed to extract valid JSON or fields
2025-04-04 15:51:26,642 - voice-service - WARNING - Regex parsing failed to extract valid JSON
2025-04-04 15:51:26,642 - voice-service - INFO - Returning response with command: None
2025-04-04 15:51:47,720 - voice-service - INFO - Received request: {'prompt': '', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 15:51:47,721 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 15:51:47,721 - voice-service - INFO - Using cached model: distilgpt2
2025-04-04 15:51:47,722 - voice-service - INFO - Created prompt: Convert this command to JSON: ""

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 15:51:47,722 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 15:51:50,091 - voice-service - INFO - Raw generation complete: {


"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 3:
Comma...
2025-04-04 15:51:50,092 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 15:51:50,092 - outlines-service - INFO - Successfully extracted JSON: {'device': 'lights', 'location': 'kitchen', 'action': 'turn_on', 'value': None}
2025-04-04 15:51:50,107 - voice-service - INFO - Returning response with command: {"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}
2025-04-04 15:51:50,236 - voice-service - INFO - Received request: {'prompt': '', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 15:51:50,236 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 15:51:50,237 - voice-service - INFO - Using cached model: distilgpt2
2025-04-04 15:51:50,237 - voice-service - INFO - Created prompt: Convert this command to JSON: ""

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 15:51:50,238 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 15:51:52,554 - voice-service - INFO - Raw generation complete: {
2025-04-04 15:51:52,555 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 15:51:52,555 - outlines-service - WARNING - Failed to extract valid JSON or fields
2025-04-04 15:51:52,555 - voice-service - WARNING - Regex parsing failed to extract valid JSON
2025-04-04 15:51:52,555 - voice-service - INFO - Returning response with command: None
2025-04-04 15:51:52,683 - voice-service - INFO - Received request: {'prompt': '', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 15:51:52,683 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 15:51:52,683 - voice-service - INFO - Using cached model: distilgpt2
2025-04-04 15:51:52,683 - voice-service - INFO - Created prompt: Convert this command to JSON: ""

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 15:51:52,683 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 15:51:54,991 - voice-service - INFO - Raw generation complete: {


}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
2025-04-04 15:51:54,992 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 15:51:54,992 - outlines-service - WARNING - Failed to extract valid JSON or fields
2025-04-04 15:51:54,992 - voice-service - WARNING - Regex parsing failed to extract valid JSON
2025-04-04 15:51:54,992 - voice-service - INFO - Returning response with command: None
2025-04-04 15:52:12,345 - voice-service - INFO - Received request: {'prompt': 'turn on the lights', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 15:52:12,345 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 15:52:12,345 - voice-service - INFO - Using cached model: distilgpt2
2025-04-04 15:52:12,346 - voice-service - INFO - Created prompt: Convert this command to JSON: "turn on the lights"

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 15:52:12,346 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 15:52:14,693 - voice-service - INFO - Raw generation complete: {
2025-04-04 15:52:14,693 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 15:52:14,693 - outlines-service - WARNING - Failed to extract valid JSON or fields
2025-04-04 15:52:14,694 - voice-service - WARNING - Regex parsing failed to extract valid JSON
2025-04-04 15:52:14,694 - voice-service - INFO - Returning response with command: None
2025-04-04 15:52:21,686 - voice-service - INFO - Received request: {'prompt': 'what is your name?', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 15:52:21,687 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 15:52:21,687 - voice-service - INFO - Using cached model: distilgpt2
2025-04-04 15:52:21,688 - voice-service - INFO - Created prompt: Convert this command to JSON: "what is your name?"

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 15:52:21,688 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 15:52:24,082 - voice-service - INFO - Raw generation complete: {


"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 3:
Comma...
2025-04-04 15:52:24,082 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 15:52:24,083 - outlines-service - INFO - Successfully extracted JSON: {'device': 'lights', 'location': 'kitchen', 'action': 'turn_on', 'value': None}
2025-04-04 15:52:24,091 - voice-service - INFO - Returning response with command: {"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}
2025-04-04 15:54:58,240 - voice-service - INFO - Received request: {'prompt': '', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 15:54:58,240 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 15:54:58,241 - voice-service - INFO - Using cached model: distilgpt2
2025-04-04 15:54:58,241 - voice-service - INFO - Created prompt: Convert this command to JSON: ""

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 15:54:58,241 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 15:55:00,832 - voice-service - INFO - Raw generation complete: {

"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 3:
Comman...
2025-04-04 15:55:00,832 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 15:55:00,833 - outlines-service - INFO - Successfully extracted JSON: {'device': 'lights', 'location': 'kitchen', 'action': 'turn_on', 'value': None}
2025-04-04 15:55:00,841 - voice-service - INFO - Returning response with command: {"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}
2025-04-04 15:55:00,978 - voice-service - INFO - Received request: {'prompt': '', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 15:55:00,978 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 15:55:00,978 - voice-service - INFO - Using cached model: distilgpt2
2025-04-04 15:55:00,979 - voice-service - INFO - Created prompt: Convert this command to JSON: ""

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 15:55:00,979 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 15:55:03,578 - voice-service - INFO - Raw generation complete: {



{
2025-04-04 15:55:03,579 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 15:55:03,579 - outlines-service - WARNING - Failed to extract valid JSON or fields
2025-04-04 15:55:03,579 - voice-service - WARNING - Regex parsing failed to extract valid JSON
2025-04-04 15:55:03,579 - voice-service - INFO - Returning response with command: None
2025-04-04 15:55:22,402 - voice-service - INFO - Unloading model: distilgpt2
2025-04-04 15:55:22,864 - voice-service - INFO - Successfully unloaded model: distilgpt2
2025-04-04 15:55:35,189 - voice-service - INFO - Requested model: tinyllama, using: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 15:55:35,189 - voice-service - INFO - Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 15:55:35,189 - voice-service - INFO - Step 1: Loading tokenizer for TinyLlama/TinyLlama-1.1B-Chat-v1.0...
2025-04-04 15:55:35,363 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 15:55:35,364 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 15:55:35,364 - voice-service - INFO - Tokenizer doesn't have vocabulary attribute, adding it...
2025-04-04 15:55:35,385 - voice-service - INFO - Added vocabulary from get_vocab with 32000 items
2025-04-04 15:55:35,385 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '</s>' and tokenizer attributes fixed.
2025-04-04 15:55:35,386 - voice-service - INFO - Step 3: Loading model weights for TinyLlama/TinyLlama-1.1B-Chat-v1.0...
2025-04-04 15:55:35,386 - voice-service - INFO - Available memory before loading model: 2.89 GB
2025-04-04 15:55:35,489 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-04 15:55:39,955 - voice-service - INFO - Model moved to device: cuda
2025-04-04 15:55:39,956 - voice-service - INFO - Available memory after loading model: 1.11 GB
2025-04-04 15:55:39,956 - voice-service - INFO - Memory used by model: 1.79 GB
2025-04-04 15:55:39,956 - voice-service - INFO - Step 3 COMPLETE: Model weights loaded.
2025-04-04 15:55:39,956 - voice-service - INFO - Successfully loaded model: TinyLlama/TinyLlama-1.1B-Chat-v1.0 (Type: llama)
2025-04-04 15:55:46,774 - voice-service - INFO - Received request: {'prompt': 'turn on the bedroom lights', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 15:55:46,774 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 15:55:46,774 - voice-service - INFO - Unloading current model TinyLlama/TinyLlama-1.1B-Chat-v1.0 to load distilgpt2
2025-04-04 15:55:46,774 - voice-service - INFO - Unloading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 15:55:47,396 - voice-service - INFO - Successfully unloaded model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 15:55:47,396 - voice-service - INFO - Loading model: distilgpt2
2025-04-04 15:55:47,397 - voice-service - INFO - Step 1: Loading tokenizer for distilgpt2...
2025-04-04 15:55:47,702 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 15:55:47,702 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 15:55:47,702 - voice-service - INFO - Tokenizer doesn't have vocabulary attribute, adding it...
2025-04-04 15:55:47,741 - voice-service - INFO - Added vocabulary from get_vocab with 50257 items
2025-04-04 15:55:47,741 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '<|endoftext|>' and tokenizer attributes fixed.
2025-04-04 15:55:47,741 - voice-service - INFO - Step 3: Loading model weights for distilgpt2...
2025-04-04 15:55:47,741 - voice-service - INFO - Available memory before loading model: 2.25 GB
2025-04-04 15:55:47,797 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-04 15:55:48,224 - voice-service - INFO - Model moved to device: cuda
2025-04-04 15:55:48,224 - voice-service - INFO - Available memory after loading model: 2.19 GB
2025-04-04 15:55:48,224 - voice-service - INFO - Memory used by model: 0.06 GB
2025-04-04 15:55:48,224 - voice-service - INFO - Step 3 COMPLETE: Model weights loaded.
2025-04-04 15:55:48,224 - voice-service - INFO - Successfully loaded model: distilgpt2 (Type: gpt2)
2025-04-04 15:55:48,225 - voice-service - INFO - Created prompt: Convert this command to JSON: "turn on the bedroom lights"

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 15:55:48,225 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 15:55:50,784 - voice-service - INFO - Raw generation complete: {
2025-04-04 15:55:50,784 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 15:55:50,785 - outlines-service - WARNING - Failed to extract valid JSON or fields
2025-04-04 15:55:50,785 - voice-service - WARNING - Regex parsing failed to extract valid JSON
2025-04-04 15:55:50,785 - voice-service - INFO - Returning response with command: None
2025-04-04 16:10:14,997 - voice-service - INFO - Using device: cuda
2025-04-04 16:10:15,016 - voice-service - INFO - GPU: Orin
2025-04-04 16:10:15,016 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 16:10:15,016 - voice-service - INFO - Using models directory: /models
2025-04-04 16:10:15,016 - voice-service - INFO - Using config directory: /app/config
2025-04-04 16:10:15,039 - voice-service - INFO - Starting server...
2025-04-04 16:10:59,353 - voice-service - INFO - Found 7 models in cache dir: {'Qwen/Qwen1.5-0.5B-Chat', 'gpt2', 'EleutherAI/pythia-70m', 'microsoft/phi-2', 'distilgpt2', 'nilq/mistral-1L-tiny', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'}
2025-04-04 16:19:07,153 - voice-service - INFO - Using device: cuda
2025-04-04 16:19:07,171 - voice-service - INFO - GPU: Orin
2025-04-04 16:19:07,172 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 16:19:07,172 - voice-service - INFO - Using models directory: /models
2025-04-04 16:19:07,172 - voice-service - INFO - Using config directory: /app/config
2025-04-04 16:19:07,195 - voice-service - INFO - Starting server...
2025-04-04 16:19:25,185 - voice-service - INFO - Found 7 models in cache dir: {'microsoft/phi-2', 'distilgpt2', 'Qwen/Qwen1.5-0.5B-Chat', 'gpt2', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'EleutherAI/pythia-70m', 'nilq/mistral-1L-tiny'}
2025-04-04 16:20:51,175 - voice-service - INFO - Using device: cuda
2025-04-04 16:20:51,193 - voice-service - INFO - GPU: Orin
2025-04-04 16:20:51,193 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 16:20:51,193 - voice-service - INFO - Using models directory: /models
2025-04-04 16:20:51,194 - voice-service - INFO - Using config directory: /app/config
2025-04-04 16:20:51,216 - voice-service - INFO - Starting server...
2025-04-04 16:20:53,600 - voice-service - INFO - Found 7 models in cache: ['EleutherAI/pythia-70m', 'distilgpt2', 'Qwen/Qwen1.5-0.5B-Chat', 'gpt2', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'nilq/mistral-1L-tiny', 'microsoft/phi-2']
2025-04-04 16:20:53,600 - voice-service - INFO - Returning 12 models: ['EleutherAI/pythia-70m', 'Qwen/Qwen1.5-0.5B-Chat', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'distilgpt2', 'gpt2', 'microsoft/phi-2', 'mistral', 'nilq/mistral-1L-tiny', 'phi2', 'pythia', 'qwen', 'tinyllama']
2025-04-04 16:21:18,229 - voice-service - INFO - Requested model: tinyllama, using: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 16:21:18,229 - voice-service - INFO - Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 16:21:18,229 - voice-service - INFO - Step 1: Loading tokenizer for TinyLlama/TinyLlama-1.1B-Chat-v1.0...
2025-04-04 16:21:18,404 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 16:21:18,404 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 16:21:18,404 - voice-service - INFO - Tokenizer doesn't have vocabulary attribute, adding it...
2025-04-04 16:21:18,427 - voice-service - INFO - Added vocabulary from get_vocab with 32000 items
2025-04-04 16:21:18,427 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '</s>' and tokenizer attributes fixed.
2025-04-04 16:21:18,428 - voice-service - INFO - Step 3: Loading model weights for TinyLlama/TinyLlama-1.1B-Chat-v1.0...
2025-04-04 16:21:18,503 - voice-service - INFO - Available memory before loading model: 4.39 GB
2025-04-04 16:21:24,760 - voice-service - ERROR - Error loading model TinyLlama/TinyLlama-1.1B-Chat-v1.0: Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):

        CUDA Setup failed despite GPU being available. Please run the following command to get more information:

        python -m bitsandbytes

        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them
        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes
        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues
2025-04-04 16:21:24,767 - voice-service - ERROR - Traceback: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py", line 1976, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/usr/local/lib/python3.10/dist-packages/transformers/integrations/bitsandbytes.py", line 21, in <module>
    import bitsandbytes as bnb
  File "/usr/local/lib/python3.10/dist-packages/bitsandbytes/__init__.py", line 6, in <module>
    from . import cuda_setup, utils, research
  File "/usr/local/lib/python3.10/dist-packages/bitsandbytes/research/__init__.py", line 1, in <module>
    from . import nn
  File "/usr/local/lib/python3.10/dist-packages/bitsandbytes/research/nn/__init__.py", line 1, in <module>
    from .modules import LinearFP8Mixed, LinearFP8Global
  File "/usr/local/lib/python3.10/dist-packages/bitsandbytes/research/nn/modules.py", line 8, in <module>
    from bitsandbytes.optim import GlobalOptimManager
  File "/usr/local/lib/python3.10/dist-packages/bitsandbytes/optim/__init__.py", line 6, in <module>
    from bitsandbytes.cextension import COMPILED_WITH_CUDA
  File "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cextension.py", line 20, in <module>
    raise RuntimeError('''
RuntimeError: 
        CUDA Setup failed despite GPU being available. Please run the following command to get more information:

        python -m bitsandbytes

        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them
        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes
        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/voice_service.py", line 289, in load_model
    model = AutoModelForCausalLM.from_pretrained(
  File "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py", line 573, in from_pretrained
    return model_class.from_pretrained(
  File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py", line 272, in _wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py", line 4292, in from_pretrained
    hf_quantizer.validate_environment(
  File "/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py", line 77, in validate_environment
    from ..integrations import validate_bnb_backend_availability
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py", line 1964, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py", line 1978, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):

        CUDA Setup failed despite GPU being available. Please run the following command to get more information:

        python -m bitsandbytes

        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them
        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes
        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues

2025-04-04 16:21:42,218 - voice-service - INFO - Requested model: phi2, using: microsoft/phi-2
2025-04-04 16:21:42,218 - voice-service - INFO - Loading model: microsoft/phi-2
2025-04-04 16:21:42,219 - voice-service - INFO - Step 1: Loading tokenizer for microsoft/phi-2...
2025-04-04 16:21:42,452 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 16:21:42,452 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 16:21:42,452 - voice-service - INFO - Tokenizer doesn't have vocabulary attribute, adding it...
2025-04-04 16:21:42,487 - voice-service - INFO - Added vocabulary from get_vocab with 50295 items
2025-04-04 16:21:42,487 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '<|endoftext|>' and tokenizer attributes fixed.
2025-04-04 16:21:42,487 - voice-service - INFO - Step 3: Loading model weights for microsoft/phi-2...
2025-04-04 16:21:42,503 - voice-service - INFO - Available memory before loading model: 4.21 GB
2025-04-04 16:21:42,526 - voice-service - ERROR - Error loading model microsoft/phi-2: Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):

        CUDA Setup failed despite GPU being available. Please run the following command to get more information:

        python -m bitsandbytes

        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them
        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes
        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues
2025-04-04 16:21:42,527 - voice-service - ERROR - Traceback: Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py", line 1976, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/usr/local/lib/python3.10/dist-packages/transformers/integrations/bitsandbytes.py", line 21, in <module>
    import bitsandbytes as bnb
  File "/usr/local/lib/python3.10/dist-packages/bitsandbytes/__init__.py", line 6, in <module>
    from . import cuda_setup, utils, research
  File "/usr/local/lib/python3.10/dist-packages/bitsandbytes/research/__init__.py", line 1, in <module>
    from . import nn
  File "/usr/local/lib/python3.10/dist-packages/bitsandbytes/research/nn/__init__.py", line 1, in <module>
    from .modules import LinearFP8Mixed, LinearFP8Global
  File "/usr/local/lib/python3.10/dist-packages/bitsandbytes/research/nn/modules.py", line 8, in <module>
    from bitsandbytes.optim import GlobalOptimManager
  File "/usr/local/lib/python3.10/dist-packages/bitsandbytes/optim/__init__.py", line 6, in <module>
    from bitsandbytes.cextension import COMPILED_WITH_CUDA
  File "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cextension.py", line 20, in <module>
    raise RuntimeError('''
RuntimeError: 
        CUDA Setup failed despite GPU being available. Please run the following command to get more information:

        python -m bitsandbytes

        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them
        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes
        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/voice_service.py", line 289, in load_model
    model = AutoModelForCausalLM.from_pretrained(
  File "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py", line 573, in from_pretrained
    return model_class.from_pretrained(
  File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py", line 272, in _wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py", line 4292, in from_pretrained
    hf_quantizer.validate_environment(
  File "/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py", line 77, in validate_environment
    from ..integrations import validate_bnb_backend_availability
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py", line 1964, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py", line 1978, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.integrations.bitsandbytes because of the following error (look up to see its traceback):

        CUDA Setup failed despite GPU being available. Please run the following command to get more information:

        python -m bitsandbytes

        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them
        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes
        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues

2025-04-04 16:24:20,802 - voice-service - INFO - Using device: cuda
2025-04-04 16:24:20,820 - voice-service - INFO - GPU: Orin
2025-04-04 16:24:20,820 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 16:24:20,820 - voice-service - INFO - Using models directory: /models
2025-04-04 16:24:20,820 - voice-service - INFO - Using config directory: /app/config
2025-04-04 16:24:20,843 - voice-service - INFO - Starting server...
2025-04-04 16:24:22,447 - voice-service - INFO - Found 7 models in cache: ['EleutherAI/pythia-70m', 'distilgpt2', 'Qwen/Qwen1.5-0.5B-Chat', 'gpt2', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'nilq/mistral-1L-tiny', 'microsoft/phi-2']
2025-04-04 16:24:22,447 - voice-service - INFO - Returning 12 models: ['EleutherAI/pythia-70m', 'Qwen/Qwen1.5-0.5B-Chat', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'distilgpt2', 'gpt2', 'microsoft/phi-2', 'mistral', 'nilq/mistral-1L-tiny', 'phi2', 'pythia', 'qwen', 'tinyllama']
2025-04-04 16:24:29,242 - voice-service - INFO - Requested model: tinyllama, using: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 16:24:29,242 - voice-service - INFO - Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 16:24:29,243 - voice-service - INFO - Step 1: Loading tokenizer for TinyLlama/TinyLlama-1.1B-Chat-v1.0...
2025-04-04 16:24:29,427 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 16:24:29,427 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 16:24:29,427 - voice-service - INFO - Tokenizer doesn't have vocabulary attribute, adding it...
2025-04-04 16:24:29,447 - voice-service - INFO - Added vocabulary from get_vocab with 32000 items
2025-04-04 16:24:29,448 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '</s>' and tokenizer attributes fixed.
2025-04-04 16:24:29,448 - voice-service - INFO - Step 3: Loading model weights for TinyLlama/TinyLlama-1.1B-Chat-v1.0...
2025-04-04 16:24:29,526 - voice-service - INFO - Available memory before loading model: 4.50 GB
2025-04-04 16:25:58,362 - voice-service - INFO - Using device: cuda
2025-04-04 16:25:58,385 - voice-service - INFO - GPU: Orin
2025-04-04 16:25:58,385 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 16:25:58,385 - voice-service - INFO - Using models directory: /models
2025-04-04 16:25:58,385 - voice-service - INFO - Using config directory: /app/config
2025-04-04 16:25:58,409 - voice-service - INFO - Starting server...
2025-04-04 16:26:06,908 - voice-service - INFO - Found 7 models in cache: ['EleutherAI/pythia-70m', 'distilgpt2', 'Qwen/Qwen1.5-0.5B-Chat', 'gpt2', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'nilq/mistral-1L-tiny', 'microsoft/phi-2']
2025-04-04 16:26:06,908 - voice-service - INFO - Returning 12 models: ['EleutherAI/pythia-70m', 'Qwen/Qwen1.5-0.5B-Chat', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'distilgpt2', 'gpt2', 'microsoft/phi-2', 'mistral', 'nilq/mistral-1L-tiny', 'phi2', 'pythia', 'qwen', 'tinyllama']
2025-04-04 16:26:26,988 - voice-service - INFO - Requested model: nilq/mistral-1L-tiny, using: nilq/mistral-1L-tiny
2025-04-04 16:26:26,988 - voice-service - INFO - Loading model: nilq/mistral-1L-tiny
2025-04-04 16:26:26,988 - voice-service - INFO - Step 1: Loading tokenizer for nilq/mistral-1L-tiny...
2025-04-04 16:26:27,147 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 16:26:27,147 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 16:26:27,147 - voice-service - INFO - Tokenizer doesn't have vocabulary attribute, adding it...
2025-04-04 16:26:27,168 - voice-service - INFO - Added vocabulary from get_vocab with 32000 items
2025-04-04 16:26:27,169 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '</s>' and tokenizer attributes fixed.
2025-04-04 16:26:27,169 - voice-service - INFO - Step 3: Loading model weights for nilq/mistral-1L-tiny...
2025-04-04 16:26:27,242 - voice-service - INFO - Available memory before loading model: 4.22 GB
2025-04-04 16:26:33,227 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-04 16:26:33,487 - voice-service - INFO - Model moved to device: cuda
2025-04-04 16:26:33,488 - voice-service - INFO - Available memory after loading model: 3.86 GB
2025-04-04 16:26:33,488 - voice-service - INFO - Memory used by model: 0.36 GB
2025-04-04 16:26:33,488 - voice-service - INFO - Step 3 COMPLETE: Model weights loaded.
2025-04-04 16:26:33,488 - voice-service - INFO - Successfully loaded model: nilq/mistral-1L-tiny (Type: mistral)
2025-04-04 16:26:45,251 - voice-service - INFO - Received request: {'prompt': 'turn on the bedroom lights', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 16:26:45,251 - voice-service - INFO - Requested model: distilgpt2, using: distilgpt2
2025-04-04 16:26:45,251 - voice-service - INFO - Unloading current model nilq/mistral-1L-tiny to load distilgpt2
2025-04-04 16:26:45,251 - voice-service - INFO - Unloading model: nilq/mistral-1L-tiny
2025-04-04 16:26:45,686 - voice-service - INFO - Successfully unloaded model: nilq/mistral-1L-tiny
2025-04-04 16:26:45,687 - voice-service - INFO - Loading model: distilgpt2
2025-04-04 16:26:45,688 - voice-service - INFO - Step 1: Loading tokenizer for distilgpt2...
2025-04-04 16:26:45,965 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 16:26:45,965 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 16:26:45,965 - voice-service - INFO - Tokenizer doesn't have vocabulary attribute, adding it...
2025-04-04 16:26:45,998 - voice-service - INFO - Added vocabulary from get_vocab with 50257 items
2025-04-04 16:26:45,999 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '<|endoftext|>' and tokenizer attributes fixed.
2025-04-04 16:26:45,999 - voice-service - INFO - Step 3: Loading model weights for distilgpt2...
2025-04-04 16:26:45,999 - voice-service - INFO - Available memory before loading model: 4.28 GB
2025-04-04 16:26:46,074 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-04 16:26:46,703 - voice-service - INFO - Model moved to device: cuda
2025-04-04 16:26:46,703 - voice-service - INFO - Available memory after loading model: 4.03 GB
2025-04-04 16:26:46,703 - voice-service - INFO - Memory used by model: 0.25 GB
2025-04-04 16:26:46,704 - voice-service - INFO - Step 3 COMPLETE: Model weights loaded.
2025-04-04 16:26:46,704 - voice-service - INFO - Successfully loaded model: distilgpt2 (Type: gpt2)
2025-04-04 16:26:46,704 - voice-service - INFO - Created prompt: Convert this command to JSON: "turn on the bedroom lights"

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Example 1:
Command: "Turn on the kitchen lights"
{"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}

Example 2:
Command: "Set thermostat to 72"
{"device": "thermostat", "location": null, "action": "set", "value": "72"}

JSON OUTPUT:

2025-04-04 16:26:46,704 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 16:26:50,569 - voice-service - INFO - Raw generation complete: {
2025-04-04 16:26:50,569 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 16:26:50,571 - outlines-service - WARNING - Failed to extract valid JSON or fields
2025-04-04 16:26:50,571 - voice-service - WARNING - Regex parsing failed to extract valid JSON
2025-04-04 16:26:50,571 - voice-service - INFO - Returning response with command: None
2025-04-04 17:06:21,092 - voice-service - INFO - Using device: cuda
2025-04-04 17:06:21,111 - voice-service - INFO - GPU: Orin
2025-04-04 17:06:21,111 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 17:06:21,111 - voice-service - INFO - Using models directory: /models
2025-04-04 17:06:21,111 - voice-service - INFO - Using config directory: /app/config
2025-04-04 17:06:21,134 - voice-service - INFO - Starting server...
2025-04-04 17:06:24,266 - voice-service - INFO - Found 7 models in cache: ['EleutherAI/pythia-70m', 'distilgpt2', 'Qwen/Qwen1.5-0.5B-Chat', 'gpt2', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'nilq/mistral-1L-tiny', 'microsoft/phi-2']
2025-04-04 17:06:24,267 - voice-service - INFO - Returning 12 models: ['EleutherAI/pythia-70m', 'Qwen/Qwen1.5-0.5B-Chat', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'distilgpt2', 'gpt2', 'microsoft/phi-2', 'mistral', 'nilq/mistral-1L-tiny', 'phi2', 'pythia', 'qwen', 'tinyllama']
2025-04-04 17:06:47,240 - voice-service - INFO - Requested model: tinyllama, using: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 17:06:47,241 - voice-service - INFO - Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 17:06:47,241 - voice-service - INFO - Step 1: Loading tokenizer for TinyLlama/TinyLlama-1.1B-Chat-v1.0...
2025-04-04 17:06:47,435 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 17:06:47,435 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 17:06:47,455 - voice-service - INFO - Added vocabulary from get_vocab with 32000 items
2025-04-04 17:06:47,456 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '</s>' and tokenizer attributes fixed.
2025-04-04 17:06:47,456 - voice-service - INFO - Step 3: Loading model weights for TinyLlama/TinyLlama-1.1B-Chat-v1.0...
2025-04-04 17:06:47,533 - voice-service - INFO - Available memory before loading model: 4.00 GB
2025-04-04 17:06:53,238 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-04 17:06:56,599 - voice-service - INFO - Model moved to device: cuda
2025-04-04 17:06:56,599 - voice-service - INFO - Available memory after loading model: 2.08 GB
2025-04-04 17:06:56,600 - voice-service - INFO - Memory used by model: 1.92 GB
2025-04-04 17:06:56,600 - voice-service - INFO - Step 3 COMPLETE: Model weights loaded.
2025-04-04 17:06:56,600 - voice-service - INFO - Successfully loaded model: TinyLlama/TinyLlama-1.1B-Chat-v1.0 (Type: llama)
2025-04-04 17:07:11,210 - voice-service - INFO - Received request: {'prompt': 'turn off bedroom lights', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 17:07:11,211 - voice-service - INFO - Requested model: tinyllama, using: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 17:07:11,211 - voice-service - INFO - Using cached model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 17:07:11,211 - voice-service - INFO - Created prompt: Convert this command to JSON: "turn off bedroom lights"

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Examples:
1. "Turn on the kitchen lights" -> {"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}
2. "Set thermostat to 72" -> {"device": "thermostat", "location": null, "action": "set", "value": "72"}
3. "Turn off the TV" -> {"device": "tv", "location": null, "action": "turn_off", "value": null}

JSON OUTPUT:

2025-04-04 17:07:11,211 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 17:07:22,112 - voice-service - INFO - Raw generation complete: {
  "device": "lights",
  "location": null,
  "action": "turn_on",
  "value": null
}

{
  "device": ...
2025-04-04 17:07:22,112 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 17:07:22,113 - outlines-service - INFO - Successfully extracted JSON: {'device': 'lights', 'location': None, 'action': 'turn_on', 'value': None}
2025-04-04 17:07:22,124 - voice-service - INFO - Returning response with command: {"device": "lights", "location": null, "action": "turn_on", "value": null}
2025-04-04 17:07:39,857 - voice-service - INFO - Unloading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 17:07:40,469 - voice-service - INFO - Successfully unloaded model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 17:07:46,541 - voice-service - INFO - Found 7 models in cache: ['EleutherAI/pythia-70m', 'distilgpt2', 'Qwen/Qwen1.5-0.5B-Chat', 'gpt2', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'nilq/mistral-1L-tiny', 'microsoft/phi-2']
2025-04-04 17:07:46,541 - voice-service - INFO - Returning 12 models: ['EleutherAI/pythia-70m', 'Qwen/Qwen1.5-0.5B-Chat', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'distilgpt2', 'gpt2', 'microsoft/phi-2', 'mistral', 'nilq/mistral-1L-tiny', 'phi2', 'pythia', 'qwen', 'tinyllama']
2025-04-04 17:08:07,256 - voice-service - INFO - Received request: {'prompt': 'laod phi2', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 17:08:07,256 - voice-service - INFO - Requested model: tinyllama, using: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 17:08:07,256 - voice-service - INFO - Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 17:08:07,257 - voice-service - INFO - Step 1: Loading tokenizer for TinyLlama/TinyLlama-1.1B-Chat-v1.0...
2025-04-04 17:08:07,438 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 17:08:07,439 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 17:08:07,460 - voice-service - INFO - Added vocabulary from get_vocab with 32000 items
2025-04-04 17:08:07,461 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '</s>' and tokenizer attributes fixed.
2025-04-04 17:08:07,461 - voice-service - INFO - Step 3: Loading model weights for TinyLlama/TinyLlama-1.1B-Chat-v1.0...
2025-04-04 17:08:07,461 - voice-service - INFO - Available memory before loading model: 2.85 GB
2025-04-04 17:08:07,533 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-04 17:08:11,710 - voice-service - INFO - Model moved to device: cuda
2025-04-04 17:08:11,712 - voice-service - INFO - Available memory after loading model: 1.71 GB
2025-04-04 17:08:11,712 - voice-service - INFO - Memory used by model: 1.14 GB
2025-04-04 17:08:11,712 - voice-service - INFO - Step 3 COMPLETE: Model weights loaded.
2025-04-04 17:08:11,712 - voice-service - INFO - Successfully loaded model: TinyLlama/TinyLlama-1.1B-Chat-v1.0 (Type: llama)
2025-04-04 17:08:11,713 - voice-service - INFO - Created prompt: Convert this command to JSON: "laod phi2"

Output format:
{
  "device": string,  // The device (lights, tv, etc.)
  "location": string | null,  // Location or null
  "action": string,  // Action (turn_on, set, etc.)
  "value": string | null  // Value or null
}

Examples:
1. "Turn on the kitchen lights" -> {"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}
2. "Set thermostat to 72" -> {"device": "thermostat", "location": null, "action": "set", "value": "72"}
3. "Turn off the TV" -> {"device": "tv", "location": null, "action": "turn_off", "value": null}

JSON OUTPUT:

2025-04-04 17:08:11,713 - voice-service - INFO - Generating raw text with temperature=0.3, max_tokens=100
2025-04-04 17:08:21,125 - voice-service - INFO - Raw generation complete: {
  "device": "lights",
  "location": "kitchen",
  "action": "turn_on",
  "value": null
}

{
  "devi...
2025-04-04 17:08:21,125 - voice-service - INFO - Using regex-based parsing for structured output...
2025-04-04 17:08:21,137 - outlines-service - INFO - Successfully extracted JSON: {'device': 'lights', 'location': 'kitchen', 'action': 'turn_on', 'value': None}
2025-04-04 17:08:21,145 - voice-service - INFO - Returning response with command: {"device": "lights", "location": "kitchen", "action": "turn_on", "value": null}
2025-04-04 17:08:29,219 - voice-service - INFO - Requested model: phi2, using: microsoft/phi-2
2025-04-04 17:08:29,220 - voice-service - INFO - Unloading current model TinyLlama/TinyLlama-1.1B-Chat-v1.0 to load microsoft/phi-2
2025-04-04 17:10:41,649 - voice-service - INFO - Using device: cuda
2025-04-04 17:10:41,668 - voice-service - INFO - GPU: Orin
2025-04-04 17:10:41,668 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 17:10:41,668 - voice-service - INFO - Using models directory: /models
2025-04-04 17:10:41,668 - voice-service - INFO - Using config directory: /app/config
2025-04-04 17:10:41,692 - voice-service - INFO - Starting server...
2025-04-04 17:10:49,972 - voice-service - INFO - Found 7 models in cache: ['EleutherAI/pythia-70m', 'distilgpt2', 'Qwen/Qwen1.5-0.5B-Chat', 'gpt2', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'nilq/mistral-1L-tiny', 'microsoft/phi-2']
2025-04-04 17:10:49,973 - voice-service - INFO - Returning 12 models: ['EleutherAI/pythia-70m', 'Qwen/Qwen1.5-0.5B-Chat', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'distilgpt2', 'gpt2', 'microsoft/phi-2', 'mistral', 'nilq/mistral-1L-tiny', 'phi2', 'pythia', 'qwen', 'tinyllama']
2025-04-04 17:11:11,289 - voice-service - INFO - Requested model: qwen, using: Qwen/Qwen1.5-0.5B-Chat
2025-04-04 17:11:11,289 - voice-service - INFO - Loading model: Qwen/Qwen1.5-0.5B-Chat
2025-04-04 17:11:11,289 - voice-service - INFO - Step 1: Loading tokenizer for Qwen/Qwen1.5-0.5B-Chat...
2025-04-04 17:11:11,969 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 17:11:11,970 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 17:11:12,102 - voice-service - INFO - Added vocabulary from get_vocab with 151646 items
2025-04-04 17:11:12,102 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '<|endoftext|>' and tokenizer attributes fixed.
2025-04-04 17:11:12,102 - voice-service - INFO - Step 3: Loading model weights for Qwen/Qwen1.5-0.5B-Chat...
2025-04-04 17:11:12,195 - voice-service - INFO - Available memory before loading model: 3.78 GB
2025-04-04 17:11:18,884 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-04 17:11:20,603 - voice-service - INFO - Model moved to device: cuda
2025-04-04 17:11:20,603 - voice-service - INFO - Available memory after loading model: 3.14 GB
2025-04-04 17:11:20,603 - voice-service - INFO - Memory used by model: 0.64 GB
2025-04-04 17:11:20,603 - voice-service - INFO - Step 3 COMPLETE: Model weights loaded.
2025-04-04 17:11:20,603 - voice-service - INFO - Successfully loaded model: Qwen/Qwen1.5-0.5B-Chat (Type: qwen2)
2025-04-04 17:11:28,122 - voice-service - INFO - Received request: {'prompt': 'turn bedroom lights on', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 17:11:28,122 - voice-service - INFO - Requested model: tinyllama, using: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 17:11:28,122 - voice-service - INFO - Unloading current model Qwen/Qwen1.5-0.5B-Chat to load TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 17:13:07,315 - voice-service - INFO - Found 7 models in cache: ['EleutherAI/pythia-70m', 'distilgpt2', 'Qwen/Qwen1.5-0.5B-Chat', 'gpt2', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'nilq/mistral-1L-tiny', 'microsoft/phi-2']
2025-04-04 17:13:07,315 - voice-service - INFO - Returning 12 models: ['Qwen/Qwen1.5-0.5B-Chat', 'EleutherAI/pythia-70m', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'distilgpt2', 'gpt2', 'microsoft/phi-2', 'mistral', 'nilq/mistral-1L-tiny', 'phi2', 'pythia', 'qwen', 'tinyllama']
2025-04-04 17:13:19,014 - voice-service - INFO - Received request: {'prompt': 'turn bedroom lights on', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 17:13:19,014 - voice-service - INFO - Requested model: tinyllama, using: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 17:16:09,471 - voice-service - INFO - Using device: cuda
2025-04-04 17:16:09,490 - voice-service - INFO - GPU: Orin
2025-04-04 17:16:09,490 - voice-service - INFO - Total GPU memory: 7.99 GB
2025-04-04 17:16:09,490 - voice-service - INFO - Using models directory: /models
2025-04-04 17:16:09,490 - voice-service - INFO - Using config directory: /app/config
2025-04-04 17:16:09,514 - voice-service - INFO - Starting server...
2025-04-04 17:16:47,246 - voice-service - INFO - Requested model: mistral, using: nilq/mistral-1L-tiny
2025-04-04 17:16:47,246 - voice-service - INFO - Loading model: nilq/mistral-1L-tiny
2025-04-04 17:16:47,246 - voice-service - INFO - Step 1: Loading tokenizer for nilq/mistral-1L-tiny...
2025-04-04 17:16:47,389 - voice-service - INFO - Step 1 COMPLETE: Tokenizer loaded.
2025-04-04 17:16:47,389 - voice-service - INFO - Step 2: Setting pad token and fixing tokenizer attributes...
2025-04-04 17:16:47,410 - voice-service - INFO - Added vocabulary from get_vocab with 32000 items
2025-04-04 17:16:47,410 - voice-service - INFO - Step 2 COMPLETE: Pad token set to '</s>' and tokenizer attributes fixed.
2025-04-04 17:16:47,410 - voice-service - INFO - Step 3: Loading model weights for nilq/mistral-1L-tiny...
2025-04-04 17:16:47,485 - voice-service - INFO - Available memory before loading model: 3.86 GB
2025-04-04 17:16:53,367 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-04 17:16:53,518 - voice-service - INFO - Model moved to device: cuda
2025-04-04 17:16:53,518 - voice-service - INFO - Available memory after loading model: 3.89 GB
2025-04-04 17:16:53,518 - voice-service - INFO - Memory used by model: -0.04 GB
2025-04-04 17:16:53,518 - voice-service - INFO - Step 3 COMPLETE: Model weights loaded.
2025-04-04 17:16:53,518 - voice-service - INFO - Successfully loaded model: nilq/mistral-1L-tiny (Type: mistral)
2025-04-04 17:17:06,204 - voice-service - INFO - Received request: {'prompt': 'turn the bedroom lights off', 'model_id': None, 'temperature': 0.3, 'max_tokens': 100}
2025-04-04 17:17:06,205 - voice-service - INFO - Requested model: tinyllama, using: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-04-04 17:17:06,205 - voice-service - INFO - Unloading current model nilq/mistral-1L-tiny to load TinyLlama/TinyLlama-1.1B-Chat-v1.0
