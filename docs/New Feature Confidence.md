# Feature Requirements: Output-Level Confidence Estimation for GBNF-Constrained JSON LLM API

## ðŸŽ¯ Goal
Extend the existing JSON-producing LLM API to include an overall `confidence` score that reflects the model's certainty in its structured output â€” while preserving the original model output for transparency.

---

## ðŸ“Œ Summary
The API response must return two structured objects:
1. `raw`: the original JSON generated by the model  
2. `final`: the same JSON, augmented with a top-level `"confidence"` field

This allows consumers to:
- Use or audit the raw output separately
- Leverage confidence metadata for downstream decision-making

---

## ðŸ“¦ Feature Requirements

### 1. **Return Dual JSON Objects**
- The API must return a dictionary with two keys: `raw` and `final`.
- `raw` is the unaltered JSON output generated by the model.
- `final` includes the same content, with one added field:  
  `"confidence"`: a discrete string from:
  - `"very_high"`, `"high"`, `"moderate"`, `"low"`, `"very_low"`

### 2. **Confidence Must Reflect Model Certainty**
- Confidence must be computed using model scoring (e.g., log-probabilities).
- Scoring must be based on the model's own evaluation of token likelihoods, before grammar constraints are applied.
- The confidence reflects the likelihood of the *entire response*, not just one token or field.

### 3. **Confidence Levels Must Be Discrete**
- Use a fixed and clearly defined set of levels:
  - `"very_high"`, `"high"`, `"moderate"`, `"low"`, `"very_low"`
- Optional: allow internal access to the raw numeric score for debugging.

### 4. **Do Not Alter Original Output**
- The `raw` output must remain structurally identical to what the model returned.
- The `final` output must only differ by the presence of the `confidence` field.

---

## âœ… Example Output
```json
{
  "raw": {
    "device": "lights",
    "action": "on",
    "location": "kitchen"
  },
  "final": {
    "device": "lights",
    "action": "on",
    "location": "kitchen",
    "confidence": "very_low"
  }
}
```

---

## ðŸ› ï¸ Implementation Approach

### Implementation Strategy: Direct llama-cpp-python Integration

Switch from llama.cpp server mode to using `llama-cpp-python` directly for access to token probabilities.

---

## ðŸ“‹ Implementation Steps

### Step 1: Create Response Wrapper
**Goal**: Structure to hold raw and final outputs
```python
class LLMResponse:
    def __init__(self, raw_json: dict):
        self.raw = raw_json
        self.final = raw_json.copy()
    
    def set_confidence(self, level: str):
        self.final["confidence"] = level
    
    def to_dict(self):
        return {"raw": self.raw, "final": self.final}
```
**Test**: Verify copying works, confidence doesn't affect raw

### Step 2: Create Model Manager (Singleton)
**Goal**: Load model once and reuse across requests
```python
from llama_cpp import Llama
import numpy as np
from typing import Optional

class ModelManager:
    _instance: Optional['ModelManager'] = None
    _model: Optional[Llama] = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def initialize(self, model_path: str, **kwargs):
        if self._model is None:
            self._model = Llama(
                model_path=model_path,
                logits_all=True,  # Enable probability tracking
                n_ctx=2048,
                n_gpu_layers=-1,  # Use all GPU layers
                **kwargs
            )
    
    @property
    def model(self) -> Llama:
        if self._model is None:
            raise RuntimeError("Model not initialized")
        return self._model

# Initialize once at startup
model_manager = ModelManager()
```
**Test**: Verify singleton behavior, model persistence

### Step 3: FastAPI Application Setup
**Goal**: Integrate model with FastAPI
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from contextlib import asynccontextmanager

class GenerateRequest(BaseModel):
    prompt: str
    grammar: Optional[str] = None
    max_tokens: int = 200

class GenerateResponse(BaseModel):
    raw: dict
    final: dict

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup: Load model once
    model_manager.initialize(
        model_path="path/to/model.gguf",
        n_threads=4
    )
    yield
    # Shutdown: Cleanup if needed

app = FastAPI(lifespan=lifespan)
```
**Test**: Verify model loads on startup
### Step 4: Confidence Calculator Service
**Goal**: Encapsulate confidence calculation logic
```python
class ConfidenceCalculator:
    @staticmethod
    def softmax(x):
        e_x = np.exp(x - np.max(x))
        return e_x / e_x.sum()
    
    def get_token_probabilities(self, output: dict) -> list:
        tokens = output["tokens"]
        logits_list = output["logits"]
        
        token_probs = []
        # Calculate prob for each generated token
        for i in range(len(tokens) - 1):
            logits = logits_list[i]
            next_token = tokens[i + 1]
            
            probs = self.softmax(np.array(logits))
            token_prob = probs[next_token]
            token_probs.append(token_prob)
        
        return token_probs
    
    def calculate_confidence_score(self, token_probs: list) -> float:
        if not token_probs:
            return 0.0
        
        # Geometric mean of probabilities
        log_probs = [np.log(p) for p in token_probs if p > 0]
        avg_log_prob = np.mean(log_probs)
        
        return np.exp(avg_log_prob)
    
    def score_to_level(self, score: float) -> str:
        if score >= 0.8:
            return "very_high"
        elif score >= 0.6:
            return "high"
        elif score >= 0.4:
            return "moderate"
        elif score >= 0.2:
            return "low"
        else:
            return "very_low"

confidence_calculator = ConfidenceCalculator()
```
**Test**: Unit test each method with mock data

### Step 5: API Endpoint Implementation
**Goal**: FastAPI endpoint with confidence estimation
```python
@app.post("/generate", response_model=GenerateResponse)
async def generate_with_confidence(request: GenerateRequest):
    try:
        # Get model instance
        model = model_manager.model
        
        # Generate with optional grammar
        output = model(
            request.prompt,
            max_tokens=request.max_tokens,
            grammar=request.grammar,
            echo=False,
            stop=["\n", "```"]
        )
        
        # Extract and parse JSON response
        generated_text = output["text"].strip()
        raw_json = json.loads(generated_text)
        
        # Calculate confidence
        token_probs = confidence_calculator.get_token_probabilities(output)
        confidence_score = confidence_calculator.calculate_confidence_score(token_probs)
        confidence_level = confidence_calculator.score_to_level(confidence_score)
        
        # Build response
        final_json = raw_json.copy()
        final_json["confidence"] = confidence_level
        
        return GenerateResponse(raw=raw_json, final=final_json)
        
    except json.JSONDecodeError:
        raise HTTPException(
            status_code=422,
            detail="Model output was not valid JSON"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Generation failed: {str(e)}"
        )
```
**Test**: Integration test with various prompts and grammars

### Step 6: Optional Debug Endpoint
**Goal**: Provide detailed probability information for calibration
```python
class DebugResponse(BaseModel):
    raw: dict
    final: dict
    debug: dict

@app.post("/generate/debug", response_model=DebugResponse)
async def generate_with_debug(request: GenerateRequest):
    # Same as generate_with_confidence but includes debug info
    # ... (generation code) ...
    
    debug_info = {
        "token_count": len(token_probs),
        "token_probabilities": token_probs[:10],  # First 10 tokens
        "avg_probability": float(np.mean(token_probs)),
        "min_probability": float(np.min(token_probs)),
        "confidence_score": float(confidence_score)
    }
    
    return DebugResponse(raw=raw_json, final=final_json, debug=debug_info)
```
**Test**: Verify debug information accuracy

### Step 7: Complete Integration Example
**Goal**: Full working example
```python
# main.py
from fastapi import FastAPI, HTTPException
from llama_cpp import Llama
import numpy as np
import json
from typing import Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager

# [Include all previous components: ModelManager, ConfidenceCalculator, etc.]

# Example usage with Home Assistant command
if __name__ == "__main__":
    import uvicorn
    
    # Example GBNF grammar for Home Assistant
    ha_grammar = """
    root ::= object
    object ::= "{" ws "\"device\":" ws device "," ws "\"action\":" ws action "," ws "\"location\":" ws location ws "}"
    device ::= "\"lights\"" | "\"thermostat\"" | "\"blinds\""
    action ::= "\"on\"" | "\"off\"" | "\"set\""
    location ::= "\"kitchen\"" | "\"bedroom\"" | "\"living_room\""
    ws ::= [ \\t\\n]*
    """
    
    uvicorn.run(app, host="0.0.0.0", port=8000)
```
**Test**: End-to-end test with curl/httpx

---

## ðŸš€ Key Implementation Notes

### Model Persistence
- **The model stays loaded in memory** between requests
- No reload overhead - weights remain in GPU VRAM
- Initial load time: 2-10 seconds (one-time cost)
- Subsequent requests: Instant (model already in memory)

### Performance Characteristics
- **llama-cpp-python speed**: Same as raw llama.cpp (C++ performance)
- **Python overhead**: < 1ms for probability calculations
- **Memory usage**: Fixed after model load (no leaks)
- **GPU optimization**: `n_gpu_layers=-1` uses all available layers

### Why This Approach
- **Direct access to logits** - Not available in server mode
- **Single process** - Easier deployment and monitoring
- **Better control** - Can tune batching, caching, threading
- **No IPC overhead** - Everything runs in your Python process

---

## ðŸ“ˆ Calibration and Tuning

The confidence thresholds (0.8, 0.6, 0.4, 0.2) may need adjustment based on your model:

```python
# Optional: Collect calibration data
confidence_scores = []
for prompt in test_prompts:
    result = generate_with_confidence(prompt, grammar)
    confidence_scores.append(result["final"]["confidence"])

# Adjust thresholds based on distribution
print(f"Average confidence: {np.mean(confidence_scores)}")
print(f"Confidence distribution: {Counter(confidence_scores)}")
```

Common adjustments:
- Smaller models â†’ Lower thresholds
- Specialized fine-tunes â†’ Tighter confidence bands
- Longer outputs â†’ Consider length normalization