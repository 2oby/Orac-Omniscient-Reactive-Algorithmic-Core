# ORAC - Open Tasks

## API Integration with Home Assistant and Procedural cleanup

The project is well-organized, with a clear separation of concerns (e.g., orac/llama_cpp_client.py, data/ for configs, models/gguf for Qwen3 0.6B, and FastAPI-related files like orac/api.py). Your goal is to enhance this project with a configuration system for Home Assistant integration, including auto-updating devices and locations, a web UI for manual edits, dynamic grammar generation, and post-processing for Home Assistant-compatible JSON outputs, all while maintaining performance (<0.2s overhead) on the Jetson Orin Nano.
Below is a step-by-step guide outlining what we want to achieve, focusing on the high-level tasks without diving into implementation details (i.e., no code). This guide builds on our previous discussions, incorporating your requirements for auto-discovery, a web UI, dynamic grammar, and post-processing with consistent percentage or absolute value mappings, tailored to your existing project structure.
Step-by-Step Guide
Step 1: Define the Configuration Structure
Create a YAML configuration file named home_assistant_config.yaml in the data/ directory to store devices, locations, actions, synonyms, and domain-specific mappings.

Structure the config to support:
Hierarchical locations (house → floor → room, e.g., bedroom, kitchen, living room).

Devices (lights, speakers, underfloor heating, blinds, TV) with Home Assistant entity IDs (e.g., light.bedroom_light).

Actions (e.g., on, off, bright, warm) with synonyms (e.g., “activate” for “on”) and parameters (e.g., brightness_pct: 70 for lights, temperature: 25 for climate).

Domain-specific mappings for actions (e.g., bright → 70% brightness for lights, warm → 25°C for underfloor heating).

Include a default error response (e.g., {"error": "Invalid command or device"}) for invalid inputs.

Ensure the config supports manual additions (e.g., custom devices) and auto-discovered entities without overwriting manual entries.

Step 2: Implement Auto-Discovery of Devices and Locations
At service startup, query Home Assistant’s REST API (/api/states) to fetch all entities (e.g., light.bedroom_light, climate.bedroom_heating).

Map entities to devices and locations based on entity ID naming conventions (e.g., light.bedroom_light → device: light, location: bedroom).

Append discovered devices and locations to the existing home_assistant_config.yaml in data/.

Preserve manually added devices/locations in the config, ensuring they are not overwritten during auto-discovery.

Store the updated config in data/home_assistant_config.yaml as the local cache.

Step 3: Generate a Dynamic JSON Grammar
At service startup, read the home_assistant_config.yaml to extract all valid devices (e.g., light, speaker), actions (e.g., turn_on, bright), and locations (e.g., bedroom, living_room).

Create a JSON grammar that restricts model outputs to a structure like {"device": str, "action": str, "location": str} with only permitted values (e.g., device ::= "light" | "speaker" | ...).

Save the grammar in memory for use by orac.llama_cpp_client.py during inference.

Update the grammar whenever the config changes (e.g., via web UI edits).

Step 4: Process User Commands with the Model
Accept raw user input (e.g., “turn on the bedroom light”, “brighten the lounge lamp”) via an API endpoint in orac/api.py.

Pass the input to orac.llama_cpp_client.py with the Qwen3 0.6B model in JSON mode, using the dynamic grammar.

Ensure the model outputs structured JSON (e.g., {"device": "light", "action": "turn_on", "location": "bedroom"}), constrained by the grammar to reduce inconsistencies (e.g., “bedroom light” vs. “light”).

Step 5: Post-Process Model Outputs
Read the model’s JSON output and map it to Home Assistant’s REST API format: {"domain": str, "service": str, "entity_id": str, "parameters": dict}.

Handle inconsistencies in the output (e.g., {"device": "bedroom light"} → {"device": "light"}) using the config’s synonyms and device mappings.

Apply domain-specific mappings for actions based on the device’s domain:
Lights: bright → brightness_pct: 70, dim → brightness_pct: 20, warm → color_temp: 250 (mireds).

Speakers/TV: loud → volume_level: 0.7, soft → volume_level: 0.3.

Underfloor Heating: warm → temperature: 25, hot → temperature: 28.

Blinds: 50%_open → position: 50.

Return the default error response (e.g., {"error": "Invalid command or device"}) if the output cannot be mapped.

Log errors to logs/ using orac.logger.

Step 6: Send Commands to Home Assistant
Use the post-processed JSON to send a REST API request to Home Assistant (e.g., POST /api/services/light/turn_on with payload {"entity_id": "light.bedroom_light", "brightness_pct": 70}).

Handle API responses, logging successes and errors to logs/.

Return the API response or error to the user via the API endpoint.

Step 7: Develop a Web UI for Configuration Management
Extend orac/api.py to include FastAPI endpoints for:
Viewing the current home_assistant_config.yaml.

Adding/editing devices, locations, actions, and synonyms.

Processing user commands (e.g., “turn on the bedroom light”).

Save UI changes to data/home_assistant_config.yaml, triggering updates to the grammar and synonym mappings.

Host the UI at http://jetson-ip:8000/docs for easy access.

Step 8: Optimize for Performance
Cache the config, synonym map, and grammar in memory to avoid disk reads during command processing.

Use dictionary-based lookups for post-processing to keep overhead <0.05s.

Leverage ORIN_NANO_OPTIMIZATIONS in orac.llama_cpp_client.py for fast inference (~0.08–0.12s with Qwen3 0.6B).

Ensure total processing time (inference + post-processing + API call) stays <0.2s.

Minimize memory usage (<1MB for config and mappings) to fit within the Jetson’s 8GB.

Step 9: Deploy and Test
Update the existing Dockerfile and docker-compose.yml in the project root to include dependencies (e.g., pyyaml, pydantic, fastapi, uvicorn, aiohttp).

Mount /Models and /Models/config (mapped to data/) in the Docker container.

Use the NVIDIA Container Toolkit for GPU acceleration.

Test the service with sample commands (e.g., “brighten the lounge lamp”, “make the bedroom heating warm”).

Verify auto-discovery appends entities correctly and preserves manual entries.

Test the web UI for config edits and command processing.

Step 10: Document and Maintain
Update README.md with setup instructions, API endpoints, and examples.

Add tests in tests/ to validate auto-discovery, grammar generation, post-processing, and API calls.

Monitor logs in logs/ for debugging.

Periodically re-run auto-discovery to catch new Home Assistant entities.

Integration with Existing Project
Config File: Place home_assistant_config.yaml in data/ alongside model_configs.yaml and grammars.yaml.

Modules: Add new files to orac/:
config_manager.py: Handle loading, validation, and auto-discovery.

grammar_generator.py: Generate dynamic grammar.

synonym_processor.py: Post-process model outputs.

ha_client.py: Manage Home Assistant API calls.

API: Extend orac/api.py for web UI endpoints.

Inference: Use existing orac.llama_cpp_client.py for Qwen3 inference.

Tests: Add test cases to tests/ for new functionality.

Scripts: Update scripts/deploy_and_test.sh to include config initialization and UI startup.

Logs: Store logs in logs/ using orac.logger.

Why This Approach
Leverages Existing Structure: Builds on your project’s data/, orac/, and models/ directories, integrating with llama_cpp_client.py and FastAPI.

Auto-Discovery: Appends Home Assistant entities without overwriting custom entries, ensuring flexibility.

Dynamic Grammar: Restricts Qwen3 outputs to valid values, reducing post-processing complexity.

Post-Processing: Standardizes outputs for Home Assistant, handling inconsistencies and applying domain-specific mappings.

Web UI: Simplifies config management for non-technical users.

Performance: Keeps processing <0.2s using in-memory caching and GPU acceleration.

Scalability: Supports hierarchical locations and future expansions (e.g., more floors).










## Immediate Tasks
- [x] Fix favoriting functionality in the UI (implemented with validation)
- [x] Set Qwen3 1.7B as the default model
- [ ] Optimize Orin GPU performance:
  - [ ] Enable maximum power mode
  - [ ] Profile GPU utilization
  - [ ] Adjust CUDA/GPU layers settings
  - [ ] Test with different batch sizes

## Future Tasks
- [ ] Add model performance benchmarks
- [ ] Create test suite for JSON command parsing
- [ ] Document model configurations and parameters
- [ ] Add model-specific error handling
- [ ] Implement model fallback strategy
- [ ] Resource Monitoring:
  - [ ] Monitor server resource usage (CPU, GPU, memory)
  - [ ] Dynamically adjust parameters based on system load
  - [ ] Integrate with psutil for memory monitoring
  - [ ] Implement adaptive resource management

# ORAC - Completed Implementation

## High-Level Approach for API-Compatible of orac.llama_cpp_client

### Completed Implementation
1. ✅ Preserve All Public Method Signatures
   - Kept generate(), list_models(), start_server(), and quantize_model() exactly as they are
   - All parameters, return types, and method names stay identical
   - External callers don't need any changes

2. ✅ Internal Server Management
   - Added private methods for server lifecycle management
   - Maintains internal state tracking current model and server status
   - Handles all server lifecycle management transparently

3. ✅ Smart Server Reuse Strategy
   - Checks if server is already running with correct model
   - Uses existing server via HTTP API if available
   - Transparently starts new server if needed
   - Caches server instances per model

4. ✅ Backward Compatible start_server() Method
   - Kept existing method signature and return type
   - Returns wrapper object that mimics subprocess.Popen
   - Tracks internal server state

5. ✅ Lazy Initialization Pattern
   - No servers started in __init__()
   - Servers only start when generate() is first called
   - Maintains lightweight initialization behavior

6. ✅ Transparent Error Handling
   - Converts HTTP errors back to same exception types/messages
   - Maintains same timeout behavior and error messages
   - External code sees no difference in error handling

7. ✅ Resource Management Strategy
   - Uses __del__() and weak references to clean up servers
   - Handles multiple LlamaCppClient instances gracefully
   - Maintains stateless appearance

8. ✅ Session Management
   - Creates aiohttp sessions lazily and reuses them
   - Cleans up sessions automatically
   - Handles session errors gracefully

Benefits Achieved:
- Zero Breaking Changes: Existing code continues to work unchanged
- Performance Gains: Internal efficiency improvements without API disruption
- Gradual Migration: Added new optional methods for advanced server control
- Risk Mitigation: Persistent server approach working well, with CLI approach as fallback

The key insight is that the efficiency gains happen at the implementation level while maintaining the same "stateless" API contract that external code expects.

## Model Configuration Tasks

### Completed
- Configured TinyLlama 1.1B for JSON command parsing:
  - System prompt with example I/O format
  - Parameters: temp=0.05, top_p=0.1, top_k=5, max_tokens=50
- Configured Qwen3 0.6B for JSON command parsing:
  - System prompt with /no_think directive
  - Parameters: temp=0.1, top_p=0.9, top_k=40, max_tokens=50
- Implemented persistent server model with automatic management
- Added server reuse strategy with caching
- Implemented lazy initialization pattern
- Added transparent error handling
- Implemented session management with aiohttp
- Implemented model selection and settings UI
- Added support for model settings persistence
- Implemented default model configuration
- Added startup model pre-loading

## Development Environment Details

### Connection Information
- Development Machine IP: (http://192.168.1.13)
- SSH Connection: `ssh orin`
- Working Directory: `/home/toby/ORAC`

### Log File System
- Log Directory: `/home/toby/ORAC/logs/`
- Current Log Files:
  - `orac.log` (current)
  - `orac.log.1` (previous)
  - `orac.log.2` (older)

