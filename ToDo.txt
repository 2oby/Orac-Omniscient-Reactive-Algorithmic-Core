ToDO

High-Level Approach for API-Compatible of orac.llama_cpp_client
1. Preserve All Public Method Signatures

Keep generate(), list_models(), start_server(), and quantize_model() exactly as they are
All parameters, return types, and method names stay identical
External callers won't need any changes

2. Internal Server Management (Hidden from API)

Add private methods like _ensure_server_running(), _start_internal_server(), _stop_internal_server()
Maintain internal state tracking current model and server status
Handle all server lifecycle management transparently within the class

3. Smart Server Reuse Strategy

In generate(): Check if server is already running with the correct model
If yes: Use existing server via HTTP API
If no/different model: Transparently start new server, then use it
Cache server instances per model to avoid restarts

4. Backward Compatible start_server() Method

Keep the existing method signature and return type (subprocess.Popen)
Internally, this could either:

Still start a separate server process for external use (maintaining old behavior)
Or return a wrapper object that mimics subprocess.Popen but tracks internal server


This ensures any existing code calling start_server() continues to work

5. Lazy Initialization Pattern

Don't start any servers in __init__()
Only start servers when generate() is first called
This maintains the current lightweight initialization behavior

6. Transparent Error Handling

Convert HTTP errors back to the same exception types/messages the CLI version would throw
Maintain the same timeout behavior and error messages
External code won't see any difference in error handling

7. Resource Management Strategy

Use __del__() or weak references to clean up servers when the client is destroyed
Handle multiple LlamaCppClient instances gracefully (different ports)
Maintain the stateless appearance - each method call should work independently

8. Session Management (Internal Only)

Create aiohttp sessions lazily and reuse them
Clean up sessions automatically without requiring explicit cleanup calls
Handle session errors gracefully by recreating them

Benefits of This Approach:

Zero Breaking Changes: Existing code continues to work unchanged
Performance Gains: Internal efficiency improvements without API disruption
Gradual Migration: Could later add new optional methods for advanced server control
Risk Mitigation: If persistent server approach has issues, easy to fall back to CLI approach

The key insight is that the efficiency gains happen at the implementation level while maintaining the same "stateless" API contract that external code expects.\


## Development Environment Details

### Connection Information
- Development Machine IP: (http://192.168.1.13)
- SSH Connection: `ssh toby@<ip-address>`
- Working Directory: `/home/toby/ORAC`

### Log File System
- Log Directory: `/home/toby/ORAC/logs/`
- Current Log Files:
  - `orac.log` (current)
  - `orac.log.1` (previous)
  - `orac.log.2` (older)

## Model Configuration Tasks

### Completed
- Configured TinyLlama 1.1B for JSON command parsing:
  - System prompt with example I/O format
  - Parameters: temp=0.05, top_p=0.1, top_k=5, max_tokens=50
- Configured Qwen3 0.6B for JSON command parsing:
  - System prompt with /no_think directive
  - Parameters: temp=0.1, top_p=0.9, top_k=40, max_tokens=50

### TODO
- [ ] Fix favoriting functionality in the UI
- [ ] Set Qwen3 1.7B as the default model
- [ ] Implement model pre-loading on startup
- [ ] Optimize Orin GPU performance:
  - [ ] Enable maximum power mode
  - [ ] Profile GPU utilization
  - [ ] Adjust CUDA/GPU layers settings
  - [ ] Test with different batch sizes

### Future Tasks
- [ ] Add model performance benchmarks
- [ ] Create test suite for JSON command parsing
- [ ] Document model configurations and parameters
- [ ] Add model-specific error handling
- [ ] Implement model fallback strategy

