# Model-specific configurations
microsoft/phi-2:
  temperature: 1.0  # Use default temperature since we're not sampling
  do_sample: false  # Disable sampling for deterministic outputs
  return_token_type_ids: false

nilq/mistral-1L-tiny:
  return_token_type_ids: false

EleutherAI/pythia-70m:
  return_token_type_ids: false
  temperature: 1.0  # Use default temperature
  do_sample: false  # Disable sampling to avoid probability issues
  num_beams: 1      # Use greedy decoding

TinyLlama/TinyLlama-1.1B-Chat-v1.0:
  return_token_type_ids: false
  temperature: 0.7  # Slightly lower temperature for more focused outputs
  do_sample: true   # Enable sampling for more natural responses

# Add more model configurations below 